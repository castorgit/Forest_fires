{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training VAE Autoencoder with ignition events (2)\n",
    "\n",
    "**Status:** PUBLIC Distribution <br>\n",
    "**File Name:** 03_Autoencoder_training_ignition_events(2).ipynb\n",
    "\n",
    "**Author:** Jaume Manero / Darshana Upadhyay / Richard Purcell<br> \n",
    "**Date created:** 2023/06/19<br>\n",
    "**Last modified:** 2023/06/19<br>\n",
    "**Description:** Autoencoders for Forest Fire prediction\n",
    "\n",
    "We train an autoencoder with a file with Ignition events. This is the first notebook. In this notebook we train the autoencoder to recognize ignition events. There is a map visualization of ignition events in BC\n",
    "\n",
    "This version uses StandardScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras.optimizers.legacy import SGD,Adam,RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'lon', 'lat', 'u10', 'v10', 'd2m', 't2m', 'e', 'cvh', 'cvl',\n",
       "       'skt', 'stl1', 'stl2', 'stl3', 'stl4', 'slt', 'sp', 'tp', 'swvl1',\n",
       "       'swvl2', 'swvl3', 'swvl4', 'month', 'day', 'hour', 'ignition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_ignition = './Data/ignition_rows.csv'\n",
    "path_non_ignition = './Data/non_ignition_rows.csv'\n",
    "ignition_df = pd.read_csv(path_ignition)  \n",
    "non_ignition_df = pd.read_csv(path_non_ignition, nrows=1000000)  \n",
    "ignition_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'lon', 'lat', 'u10', 'v10', 'd2m', 't2m', 'e', 'cvh', 'cvl',\n",
       "       'skt', 'stl1', 'stl2', 'stl3', 'stl4', 'slt', 'sp', 'tp', 'swvl1',\n",
       "       'swvl2', 'swvl3', 'swvl4', 'month', 'day', 'hour', 'ignition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_ignition_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignition_df.drop(['lon', 'lat', 'date', 'ignition'], axis=1, inplace=True)\n",
    "non_ignition_df.drop(['lon', 'lat', 'date', 'ignition'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignition shape (51918, 22)\n",
      "non_ignition shape (1000000, 22)\n"
     ]
    }
   ],
   "source": [
    "# we create a numpy array with the features of every row\n",
    "ignition_np = ignition_df.to_numpy()\n",
    "print('ignition shape',ignition_np.shape)\n",
    "non_ignition_np = non_ignition_df.to_numpy()\n",
    "print('non_ignition shape',non_ignition_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manero/DL/lib/python3.8/site-packages/numpy/core/_methods.py:235: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(51918, 22)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignition_df = ignition_df.replace([np.inf, -np.inf], 0)\n",
    "scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "scaler = PowerTransformer()\n",
    "ignition_scaled = scaler.fit_transform(ignition_df)\n",
    "ignition_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 22)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_ignition_df = non_ignition_df.replace([np.inf, -np.inf], 0)\n",
    "scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "scaler = PowerTransformer()\n",
    "non_ignition_scaled = scaler.fit_transform(non_ignition_df)\n",
    "non_ignition_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41534, 22)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate train set\n",
    "# training set will consist of ignition dataset\n",
    "\n",
    "len_ignition = len(ignition_scaled)\n",
    "len_ignition_train = int(0.80 * len_ignition)\n",
    "X_train = ignition_scaled[:len_ignition_train]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 22)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate non-ignition set\n",
    "# test will be only non-ignition data (much larger)\n",
    "\n",
    "len_non_ignition = len(non_ignition_scaled)\n",
    "X_test = non_ignition_scaled\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653\n"
     ]
    }
   ],
   "source": [
    "#remove Nans and convert them to 0\n",
    "print(np.count_nonzero(np.isnan(X_train)))\n",
    "X_train = np.nan_to_num(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988597\n"
     ]
    }
   ],
   "source": [
    "#remove Nans and convert them to 0\n",
    "print(np.count_nonzero(np.isnan(X_test)))\n",
    "X_test = np.nan_to_num(X_test)\n",
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2, axis=1))\n",
    "    #return MAE\n",
    "    return np.mean(np.abs(v1 - v2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reparameterization trick\n",
    "\n",
    "def sample(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = X_train.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "intermediate_dim = int(original_dim / 2)\n",
    "latent_dim = int(original_dim / 4)\n",
    "intermediate_dim = 32\n",
    "latent_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           736         ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           528         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 16)           272         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 4)            68          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 4)            68          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 4)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,672\n",
      "Trainable params: 1,672\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x      = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "x      = Dense(intermediate_dim/2, activation='relu')(x)\n",
    "x      = Dense(intermediate_dim/2, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# use the reparameterization trick and get the output from the sample() function\n",
    "z        = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder  = Model(inputs, z, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                80        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                544       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 22)                726       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,406\n",
      "Trainable params: 2,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x             = Dense(intermediate_dim/2, activation='relu')(latent_inputs)\n",
    "x             = Dense(intermediate_dim, activation='relu')(x)\n",
    "x             = Dense(intermediate_dim, activation='relu')(x)\n",
    "outputs       = Dense(original_dim, activation='sigmoid')(x)\n",
    "# Instantiate the decoder model:\n",
    "decoder       = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full VAE model\n",
    "outputs   = decoder(encoder(inputs))\n",
    "vae_model = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the KL loss function:\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # compute the average MSE error, then scale it up, ie. simply sum on all axes\n",
    "    reconstruction_loss = K.sum(K.square(x - x_decoded_mean))\n",
    "    # compute the KL loss\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.square(K.exp(z_log_var)), axis=-1)\n",
    "    # return the average loss over all \n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss)    \n",
    "    #total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 22)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 4)                 1672      \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 22)                2406      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,078\n",
      "Trainable params: 4,078\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 41534 samples\n",
      "Epoch 1/5000\n",
      "41534/41534 [==============================] - 1s 19us/sample - loss: 2775.3317\n",
      "Epoch 2/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2399.2613\n",
      "Epoch 3/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2260.1055\n",
      "Epoch 4/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2218.4693\n",
      "Epoch 5/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2203.6409\n",
      "Epoch 6/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2194.1198\n",
      "Epoch 7/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2186.7561\n",
      "Epoch 8/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2180.0445\n",
      "Epoch 9/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2175.1911\n",
      "Epoch 10/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2171.6764\n",
      "Epoch 11/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2167.8906\n",
      "Epoch 12/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2164.5038\n",
      "Epoch 13/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2161.2294\n",
      "Epoch 14/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2159.1467\n",
      "Epoch 15/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2156.7349\n",
      "Epoch 16/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2154.9139\n",
      "Epoch 17/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2153.0866\n",
      "Epoch 18/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2151.1687\n",
      "Epoch 19/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2149.7627\n",
      "Epoch 20/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2147.9846\n",
      "Epoch 21/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2145.6912\n",
      "Epoch 22/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2140.6534\n",
      "Epoch 23/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2136.3378\n",
      "Epoch 24/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2133.1428\n",
      "Epoch 25/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2130.3973\n",
      "Epoch 26/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2128.6133\n",
      "Epoch 27/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2126.6021\n",
      "Epoch 28/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 2124.6789\n",
      "Epoch 29/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2123.0337\n",
      "Epoch 30/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2120.8830\n",
      "Epoch 31/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2119.1480\n",
      "Epoch 32/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2117.7919\n",
      "Epoch 33/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2115.9427\n",
      "Epoch 34/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2114.2831\n",
      "Epoch 35/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 2112.7569\n",
      "Epoch 36/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 2110.5438\n",
      "Epoch 37/5000\n",
      "41534/41534 [==============================] - 1s 16us/sample - loss: 2107.7086\n",
      "Epoch 38/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 2106.1226\n",
      "Epoch 39/5000\n",
      "41534/41534 [==============================] - 1s 16us/sample - loss: 2104.2081\n",
      "Epoch 40/5000\n",
      "41534/41534 [==============================] - 1s 16us/sample - loss: 2102.1251\n",
      "Epoch 41/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 2100.0719\n",
      "Epoch 42/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 2099.3120\n",
      "Epoch 43/5000\n",
      "41534/41534 [==============================] - 1s 17us/sample - loss: 2097.0521\n",
      "Epoch 44/5000\n",
      "41534/41534 [==============================] - 1s 19us/sample - loss: 2095.6670\n",
      "Epoch 45/5000\n",
      "41534/41534 [==============================] - 1s 20us/sample - loss: 2093.6437\n",
      "Epoch 46/5000\n",
      "41534/41534 [==============================] - 1s 18us/sample - loss: 2093.2676\n",
      "Epoch 47/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 2091.7067\n",
      "Epoch 48/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2090.2274\n",
      "Epoch 49/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2075.7104\n",
      "Epoch 50/5000\n",
      "41534/41534 [==============================] - 1s 17us/sample - loss: 2044.8504\n",
      "Epoch 51/5000\n",
      "41534/41534 [==============================] - 1s 16us/sample - loss: 2042.4407\n",
      "Epoch 52/5000\n",
      "41534/41534 [==============================] - 1s 17us/sample - loss: 2040.9488\n",
      "Epoch 53/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 2039.7633\n",
      "Epoch 54/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2038.7257\n",
      "Epoch 55/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 2038.2610\n",
      "Epoch 56/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2036.9004\n",
      "Epoch 57/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 2035.3721\n",
      "Epoch 58/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2034.2256\n",
      "Epoch 59/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2032.9886\n",
      "Epoch 60/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 2031.7724\n",
      "Epoch 61/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2031.3655\n",
      "Epoch 62/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2029.0467\n",
      "Epoch 63/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2028.7059\n",
      "Epoch 64/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 2028.6583\n",
      "Epoch 65/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2027.0724\n",
      "Epoch 66/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 2026.3476\n",
      "Epoch 67/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2024.8931\n",
      "Epoch 68/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 2024.8533\n",
      "Epoch 69/5000\n",
      "41534/41534 [==============================] - 1s 16us/sample - loss: 2023.5326\n",
      "Epoch 70/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 2022.9631\n",
      "Epoch 71/5000\n",
      "41534/41534 [==============================] - 1s 16us/sample - loss: 2022.8117\n",
      "Epoch 72/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 2022.5738\n",
      "Epoch 73/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 2021.4267\n",
      "Epoch 74/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 2021.1874\n",
      "Epoch 75/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 2020.1068\n",
      "Epoch 76/5000\n",
      "41534/41534 [==============================] - 1s 15us/sample - loss: 1992.6895\n",
      "Epoch 77/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 1981.1390\n",
      "Epoch 78/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 1979.8140\n",
      "Epoch 79/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1978.5582\n",
      "Epoch 80/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 1977.1116\n",
      "Epoch 81/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 1975.7207\n",
      "Epoch 82/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 1976.0018\n",
      "Epoch 83/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 1974.7690\n",
      "Epoch 84/5000\n",
      "41534/41534 [==============================] - 1s 14us/sample - loss: 1974.0489\n",
      "Epoch 85/5000\n",
      "41534/41534 [==============================] - 1s 16us/sample - loss: 1973.2783\n",
      "Epoch 86/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 1961.9539\n",
      "Epoch 87/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 1917.5187\n",
      "Epoch 88/5000\n",
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1915.9520\n",
      "Epoch 89/5000\n",
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1897.3782\n",
      "Epoch 90/5000\n",
      "41534/41534 [==============================] - 0s 10us/sample - loss: 1880.6850\n",
      "Epoch 91/5000\n",
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1876.4865\n",
      "Epoch 92/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 1874.1253\n",
      "Epoch 93/5000\n",
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1872.9615\n",
      "Epoch 94/5000\n",
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1871.8195\n",
      "Epoch 95/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 1870.7652\n",
      "Epoch 96/5000\n",
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1869.4339\n",
      "Epoch 97/5000\n",
      "41534/41534 [==============================] - 1s 12us/sample - loss: 1868.0000\n",
      "Epoch 98/5000\n",
      "41534/41534 [==============================] - 1s 13us/sample - loss: 1866.6209\n",
      "Epoch 99/5000\n",
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1867.1435\n",
      "Epoch 100/5000\n",
      "41534/41534 [==============================] - 0s 12us/sample - loss: 1867.0695\n",
      "Epoch 101/5000\n",
      "41534/41534 [==============================] - 0s 11us/sample - loss: 1866.5607\n",
      "Epoch 102/5000\n",
      "23808/41534 [================>.............] - ETA: 0s - loss: 1868.3619"
     ]
    }
   ],
   "source": [
    "opt = RMSprop(learning_rate=0.0001)\n",
    "opt = Adam(learning_rate=0.00001, clipvalue=0.5)\n",
    "opt = Adam()\n",
    "\n",
    "\n",
    "vae_model.compile(optimizer=opt, loss=vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "trained = False     # First time train / Next times use saved model to speed up analysis\n",
    "if trained:\n",
    "    vae_model = load_model('./Data/vae_model.hf5', custom_objects={'vae_loss': vae_loss})\n",
    "\n",
    "else:\n",
    "    results = vae_model.fit(X_train, X_train,\n",
    "                        shuffle=True,\n",
    "                        epochs=5000,\n",
    "                        batch_size=128)\n",
    "    # Training figure\n",
    "    plt.plot(results.history['loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right');\n",
    "    plt.show()\n",
    "    # saving weights\n",
    "    vae_model.save('./Data/vae_model.hf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pred = vae_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_vector_train = get_error_term(X_train_pred, X_train, _rmse=False)\n",
    "\n",
    "print(f'Avg error {np.mean(mae_vector_train)}\\nmedian error {np.median(mae_vector_train)}\\n99Q:    \\\n",
    "      {np.quantile(mae_vector_train, 0.99)}')\n",
    "print(f'setting threshold on { np.quantile(mae_vector_train, 0.99)} ')\n",
    "\n",
    "error_thresh = np.quantile(mae_vector_train, 0.99)\n",
    "error_thresh = np.quantile(mae_vector_train, 0.995)\n",
    "error_median = np.quantile(mae_vector_train, 0.5)\n",
    "print(error_thresh, error_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "ax.set_title('Distribution Plot reconstruction error ignitions')\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "plt.xlim(0,0.2)\n",
    "plt.ylim(0,50)\n",
    "sns.histplot(data=mae_vector_train, bins = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_test = vae_model.predict(X_test)\n",
    "print('threshold', error_thresh,error_median)\n",
    "#train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
    "\n",
    "mae_vector = get_error_term(X_pred_test, X_test, _rmse=False)\n",
    "anomalies = (mae_vector > error_median)\n",
    "\n",
    "np.count_nonzero(anomalies) / len(anomalies)\n",
    "print('total non_ignitions',len(X_pred_test))\n",
    "print('number of almost_ignitions', np.count_nonzero(anomalies==0))\n",
    "\n",
    "#print('total y_test',len(y_test))\n",
    "#print(np.count_nonzero(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1  = plt.subplots(figsize=(7,6))\n",
    "sns.histplot(data=mae_vector, ax=ax1, color='skyblue', label='non-ignition', kde=True)\n",
    "sns.histplot(data=mae_vector_train, ax=ax1, color='red', label='ignition')\n",
    "xmin, xmax, ymin, ymax = plt.axis()\n",
    "liney = np.arange(ymin, ymax, 0.01)\n",
    "linex = np.ones(len(liney)) * error_median\n",
    "sns.lineplot(x = linex, y= liney, color='black', ax=ax1, linewidth=3, estimator = None, label='error mean')\n",
    "ax.set_title('Comparison reconstruction error ignition / non-ignition')\n",
    "ax.yaxis.set_major_formatter(StrMethodFormatter('{x:,.0f}'))\n",
    "#plt.xlim(0,0.2)\n",
    "#plt.ylim(0,8000)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_error = 0.175\n",
    "near_ignitions = (mae_vector > threshold_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second prediction with X_test vector to be used in PCA transformation\n",
    "X_pred2 = vae_model.predict(X_test)\n",
    "# PCA transformation to observe variance in 2 components\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_transform = pca.fit_transform(X_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure comparison PCA transformations\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "fig.suptitle('PCA transformations explanation',size=20)\n",
    "\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=mae_vector, ax=ax1)\n",
    "ax1.set_xlabel('MAE Vector', size=16)\n",
    "\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=near_ignitions, ax=ax2)\n",
    "ax2.set_xlabel('Near ignitions', size = 16)\n",
    "legend_labels2, _ = ax2.get_legend_handles_labels()\n",
    "ax2.legend(legend_labels2, ['Ignition', 'non-ignition'], title='Near Ignitions')\n",
    "\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=10, hue=near_ignitions, ax=ax3)\n",
    "ax3.set_xlabel('y_test', size = 16)\n",
    "legend_labels3, _ = ax3.get_legend_handles_labels()\n",
    "ax3.legend(legend_labels3, ['Ignition', 'non-ignition'], title='Near ignitions')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===== program ends here. now create a new label and label non-ignition dataset with new class ===== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
