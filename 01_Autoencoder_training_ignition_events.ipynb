{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Autoencoder with ignition events\n",
    "\n",
    "**Status:** PUBLIC Distribution <br>\n",
    "**File Name:** 01_Autoencoder_training_ignition_events.ipynb\n",
    "\n",
    "**Author:** Jaume Manero / Darshana Upadhyay / Richard Purcell<br> \n",
    "**Date created:** 2023/06/19<br>\n",
    "**Last modified:** 2023/06/19<br>\n",
    "**Description:** Autoencoders for Forest Fire prediction\n",
    "\n",
    "We train an autoencoder with a file with Ignition events. This is the first notebook. In this notebook we train the autoencoder to recognize ignition events. There is a map visualization of ignition events in BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras.optimizers.legacy import SGD,Adam,RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "NOISE = 0.5     # amount of noise to add to exercise 0.2\n",
    "NETWORK = 'ANN'    # or ANN\n",
    "THRESHOLD = 3      # 1: mean, 2: max, 3: deterministic\n",
    "threshold_d = 0.06 # threshold for deterministic 0.05 and for min and max : 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'lon', 'lat', 'u10', 'v10', 'd2m', 't2m', 'e', 'cvh', 'cvl',\n",
       "       'skt', 'stl1', 'stl2', 'stl3', 'stl4', 'slt', 'sp', 'tp', 'swvl1',\n",
       "       'swvl2', 'swvl3', 'swvl4', 'month', 'day', 'hour', 'ignition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_ignition = './Data/ignition_rows.csv'\n",
    "path_non_ignition = './Data/non_ignition_rows.csv'\n",
    "ignition_df = pd.read_csv(path_ignition)  \n",
    "non_ignition_df = pd.read_csv(path_non_ignition, nrows=1000000)  \n",
    "ignition_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'lon', 'lat', 'u10', 'v10', 'd2m', 't2m', 'e', 'cvh', 'cvl',\n",
       "       'skt', 'stl1', 'stl2', 'stl3', 'stl4', 'slt', 'sp', 'tp', 'swvl1',\n",
       "       'swvl2', 'swvl3', 'swvl4', 'month', 'day', 'hour', 'ignition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_ignition_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignition_df.drop(['lon', 'lat', 'date', 'ignition'], axis=1, inplace=True)\n",
    "non_ignition_df.drop(['lon', 'lat', 'date', 'ignition'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignition shape (51918, 22)\n",
      "non_ignition shape (1000000, 22)\n"
     ]
    }
   ],
   "source": [
    "# we create a numpy array with the features of every row\n",
    "ignition_np = ignition_df.to_numpy()\n",
    "print('ignition shape',ignition_np.shape)\n",
    "non_ignition_np = non_ignition_df.to_numpy()\n",
    "print('non_ignition shape',non_ignition_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51918, 22)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ignition_df = ignition_df.replace([np.inf, -np.inf], 0)\n",
    "scaler = MinMaxScaler()\n",
    "ignition_scaled = scaler.fit_transform(ignition_df)\n",
    "ignition_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 22)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_ignition_df = non_ignition_df.replace([np.inf, -np.inf], 0)\n",
    "scaler = MinMaxScaler()\n",
    "non_ignition_scaled = scaler.fit_transform(non_ignition_df)\n",
    "non_ignition_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41534, 22)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate train set\n",
    "# training set will consist of ignition dataset\n",
    "\n",
    "len_ignition = len(ignition_scaled)\n",
    "len_ignition_train = int(0.80 * len_ignition)\n",
    "X_train = ignition_scaled[:len_ignition_train]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 22)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate test set\n",
    "# training set will consist of ignition dataset\n",
    "\n",
    "len_non_ignition = len(non_ignition_scaled)\n",
    "X_test = non_ignition_scaled\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#remove Nans and convert them to 0\n",
    "print(np.count_nonzero(np.isnan(X_train)))\n",
    "X_train = np.nan_to_num(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1988597\n"
     ]
    }
   ],
   "source": [
    "#remove Nans and convert them to 0\n",
    "print(np.count_nonzero(np.isnan(X_test)))\n",
    "X_test = np.nan_to_num(X_test)\n",
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2, axis=1))\n",
    "    #return MAE\n",
    "    return np.mean(abs(v1 - v2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reparameterization trick\n",
    "\n",
    "def sample(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = X_train.shape[1]\n",
    "input_shape = (original_dim,)\n",
    "intermediate_dim = int(original_dim / 2)\n",
    "latent_dim = int(original_dim / 3)\n",
    "intermediate_dim = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_input (InputLayer)     [(None, 22)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2048)         47104       ['encoder_input[0][0]']          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         2098176     ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 7)            7175        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 7)            7175        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " z (Lambda)                     (None, 7)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,159,630\n",
      "Trainable params: 2,159,630\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x      = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "x      = Dense(intermediate_dim/2, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "# use the reparameterization trick and get the output from the sample() function\n",
    "z        = Lambda(sample, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "encoder  = Model(inputs, z, name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " z_sampling (InputLayer)     [(None, 7)]               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2048)              16384     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 22)                22550     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,137,110\n",
      "Trainable params: 2,137,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x             = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "x             = Dense(intermediate_dim/2, activation='relu')(x)\n",
    "outputs       = Dense(original_dim, activation='sigmoid')(x)\n",
    "# Instantiate the decoder model:\n",
    "decoder       = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full VAE model\n",
    "outputs   = decoder(encoder(inputs))\n",
    "vae_model = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the KL loss function:\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    # compute the average MSE error, then scale it up, ie. simply sum on all axes\n",
    "    reconstruction_loss = K.sum(K.square(x - x_decoded_mean))\n",
    "    # compute the KL loss\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.square(K.exp(z_log_var)), axis=-1)\n",
    "    # return the average loss over all \n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss)    \n",
    "    #total_loss = reconstruction_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vae_mlp\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_input (InputLayer)  [(None, 22)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 7)                 2159630   \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 22)                2137110   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,296,740\n",
      "Trainable params: 4,296,740\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 41534 samples\n",
      "Epoch 1/500\n",
      "41534/41534 [==============================] - 20s 486us/sample - loss: 46.6911\n",
      "Epoch 2/500\n",
      "41534/41534 [==============================] - 16s 389us/sample - loss: 25.7168\n",
      "Epoch 3/500\n",
      "41534/41534 [==============================] - 21s 496us/sample - loss: 22.2970\n",
      "Epoch 4/500\n",
      "41534/41534 [==============================] - 19s 456us/sample - loss: 20.9016\n",
      "Epoch 5/500\n",
      "41534/41534 [==============================] - 17s 420us/sample - loss: 20.0209\n",
      "Epoch 6/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 19.4810\n",
      "Epoch 7/500\n",
      "41534/41534 [==============================] - 19s 460us/sample - loss: 19.1313\n",
      "Epoch 8/500\n",
      "41534/41534 [==============================] - 19s 453us/sample - loss: 18.8357\n",
      "Epoch 9/500\n",
      "41534/41534 [==============================] - 19s 459us/sample - loss: 18.6116\n",
      "Epoch 10/500\n",
      "41534/41534 [==============================] - 19s 462us/sample - loss: 18.3929\n",
      "Epoch 11/500\n",
      "41534/41534 [==============================] - 17s 419us/sample - loss: 18.2380\n",
      "Epoch 12/500\n",
      "41534/41534 [==============================] - 17s 409us/sample - loss: 18.0969\n",
      "Epoch 13/500\n",
      "41534/41534 [==============================] - 18s 426us/sample - loss: 17.9873\n",
      "Epoch 14/500\n",
      "41534/41534 [==============================] - 17s 409us/sample - loss: 17.8786\n",
      "Epoch 15/500\n",
      "41534/41534 [==============================] - 18s 423us/sample - loss: 17.7627\n",
      "Epoch 16/500\n",
      "41534/41534 [==============================] - 17s 408us/sample - loss: 17.6794\n",
      "Epoch 17/500\n",
      "41534/41534 [==============================] - 20s 471us/sample - loss: 17.5914\n",
      "Epoch 18/500\n",
      "41534/41534 [==============================] - 20s 492us/sample - loss: 17.5501\n",
      "Epoch 19/500\n",
      "41534/41534 [==============================] - 17s 414us/sample - loss: 17.4716\n",
      "Epoch 20/500\n",
      "41534/41534 [==============================] - 17s 418us/sample - loss: 17.4330\n",
      "Epoch 21/500\n",
      "41534/41534 [==============================] - 17s 408us/sample - loss: 17.3666\n",
      "Epoch 22/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 17.3182\n",
      "Epoch 23/500\n",
      "41534/41534 [==============================] - 17s 399us/sample - loss: 17.2550\n",
      "Epoch 24/500\n",
      "41534/41534 [==============================] - 16s 386us/sample - loss: 17.2340\n",
      "Epoch 25/500\n",
      "41534/41534 [==============================] - 16s 383us/sample - loss: 17.2030\n",
      "Epoch 26/500\n",
      "41534/41534 [==============================] - 16s 384us/sample - loss: 17.1347\n",
      "Epoch 27/500\n",
      "41534/41534 [==============================] - 17s 413us/sample - loss: 17.0849\n",
      "Epoch 28/500\n",
      "41534/41534 [==============================] - 21s 500us/sample - loss: 17.0832\n",
      "Epoch 29/500\n",
      "41534/41534 [==============================] - 17s 419us/sample - loss: 17.0337\n",
      "Epoch 30/500\n",
      "41534/41534 [==============================] - 17s 421us/sample - loss: 16.9873\n",
      "Epoch 31/500\n",
      "41534/41534 [==============================] - 17s 412us/sample - loss: 16.9920\n",
      "Epoch 32/500\n",
      "41534/41534 [==============================] - 16s 395us/sample - loss: 16.9156\n",
      "Epoch 33/500\n",
      "41534/41534 [==============================] - 17s 401us/sample - loss: 16.9017\n",
      "Epoch 34/500\n",
      "41534/41534 [==============================] - 20s 485us/sample - loss: 16.8913\n",
      "Epoch 35/500\n",
      "41534/41534 [==============================] - 21s 510us/sample - loss: 16.8572\n",
      "Epoch 36/500\n",
      "41534/41534 [==============================] - 19s 469us/sample - loss: 16.8436\n",
      "Epoch 37/500\n",
      "41534/41534 [==============================] - 17s 416us/sample - loss: 16.8127\n",
      "Epoch 38/500\n",
      "41534/41534 [==============================] - 22s 528us/sample - loss: 16.7865\n",
      "Epoch 39/500\n",
      "41534/41534 [==============================] - 20s 476us/sample - loss: 16.7633\n",
      "Epoch 40/500\n",
      "41534/41534 [==============================] - 18s 439us/sample - loss: 16.7434\n",
      "Epoch 41/500\n",
      "41534/41534 [==============================] - 17s 402us/sample - loss: 16.7416\n",
      "Epoch 42/500\n",
      "41534/41534 [==============================] - 19s 462us/sample - loss: 16.7090\n",
      "Epoch 43/500\n",
      "41534/41534 [==============================] - 25s 592us/sample - loss: 16.6725\n",
      "Epoch 44/500\n",
      "41534/41534 [==============================] - 21s 507us/sample - loss: 16.6994\n",
      "Epoch 45/500\n",
      "41534/41534 [==============================] - 19s 460us/sample - loss: 16.6173\n",
      "Epoch 46/500\n",
      "41534/41534 [==============================] - 21s 512us/sample - loss: 16.6288\n",
      "Epoch 47/500\n",
      "41534/41534 [==============================] - 21s 496us/sample - loss: 16.6110\n",
      "Epoch 48/500\n",
      "41534/41534 [==============================] - 20s 477us/sample - loss: 16.5871\n",
      "Epoch 49/500\n",
      "41534/41534 [==============================] - 17s 414us/sample - loss: 16.5591\n",
      "Epoch 50/500\n",
      "41534/41534 [==============================] - 18s 425us/sample - loss: 16.5430\n",
      "Epoch 51/500\n",
      "41534/41534 [==============================] - 17s 406us/sample - loss: 16.5283\n",
      "Epoch 52/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.5463\n",
      "Epoch 53/500\n",
      "41534/41534 [==============================] - 19s 449us/sample - loss: 16.5037\n",
      "Epoch 54/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.4939\n",
      "Epoch 55/500\n",
      "41534/41534 [==============================] - 19s 450us/sample - loss: 16.5032\n",
      "Epoch 56/500\n",
      "41534/41534 [==============================] - 17s 410us/sample - loss: 16.4921\n",
      "Epoch 57/500\n",
      "41534/41534 [==============================] - 20s 492us/sample - loss: 16.4966\n",
      "Epoch 58/500\n",
      "41534/41534 [==============================] - 19s 457us/sample - loss: 16.4246\n",
      "Epoch 59/500\n",
      "41534/41534 [==============================] - 19s 449us/sample - loss: 16.4310\n",
      "Epoch 60/500\n",
      "41534/41534 [==============================] - 19s 451us/sample - loss: 16.4384\n",
      "Epoch 61/500\n",
      "41534/41534 [==============================] - 19s 450us/sample - loss: 16.3939\n",
      "Epoch 62/500\n",
      "41534/41534 [==============================] - 19s 450us/sample - loss: 16.3997\n",
      "Epoch 63/500\n",
      "41534/41534 [==============================] - 19s 454us/sample - loss: 16.3882\n",
      "Epoch 64/500\n",
      "41534/41534 [==============================] - 21s 512us/sample - loss: 16.3763\n",
      "Epoch 65/500\n",
      "41534/41534 [==============================] - 22s 525us/sample - loss: 16.3792\n",
      "Epoch 66/500\n",
      "41534/41534 [==============================] - 20s 492us/sample - loss: 16.3556\n",
      "Epoch 67/500\n",
      "41534/41534 [==============================] - 19s 456us/sample - loss: 16.3643\n",
      "Epoch 68/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 16.3492\n",
      "Epoch 69/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.3617\n",
      "Epoch 70/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.3235\n",
      "Epoch 71/500\n",
      "41534/41534 [==============================] - 19s 449us/sample - loss: 16.3164\n",
      "Epoch 72/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.2867\n",
      "Epoch 73/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.2906\n",
      "Epoch 74/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.2823\n",
      "Epoch 75/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.2925\n",
      "Epoch 76/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.2694\n",
      "Epoch 77/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.2597\n",
      "Epoch 78/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 16.2422\n",
      "Epoch 79/500\n",
      "41534/41534 [==============================] - 19s 448us/sample - loss: 16.2295\n",
      "Epoch 80/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.2287\n",
      "Epoch 81/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.2390\n",
      "Epoch 82/500\n",
      "41534/41534 [==============================] - 18s 439us/sample - loss: 16.2359\n",
      "Epoch 83/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.2266\n",
      "Epoch 84/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.2224\n",
      "Epoch 85/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.1728\n",
      "Epoch 86/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1774\n",
      "Epoch 87/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.2049\n",
      "Epoch 88/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1778\n",
      "Epoch 89/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.1772\n",
      "Epoch 90/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1636\n",
      "Epoch 91/500\n",
      "41534/41534 [==============================] - 18s 439us/sample - loss: 16.1689\n",
      "Epoch 92/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1656\n",
      "Epoch 93/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1490\n",
      "Epoch 94/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1488\n",
      "Epoch 95/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1471\n",
      "Epoch 96/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1133\n",
      "Epoch 97/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1229\n",
      "Epoch 98/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1418\n",
      "Epoch 99/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1116\n",
      "Epoch 100/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0941\n",
      "Epoch 101/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.1010\n",
      "Epoch 102/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1136\n",
      "Epoch 103/500\n",
      "41534/41534 [==============================] - 19s 449us/sample - loss: 16.0982\n",
      "Epoch 104/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1003\n",
      "Epoch 105/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0890\n",
      "Epoch 106/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0929\n",
      "Epoch 107/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.0909\n",
      "Epoch 108/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0638\n",
      "Epoch 109/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0540\n",
      "Epoch 110/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0623\n",
      "Epoch 111/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0556\n",
      "Epoch 112/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0555\n",
      "Epoch 113/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0501\n",
      "Epoch 114/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0959\n",
      "Epoch 115/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.0288\n",
      "Epoch 116/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0376\n",
      "Epoch 117/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0482\n",
      "Epoch 118/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0354\n",
      "Epoch 119/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0314\n",
      "Epoch 120/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0487\n",
      "Epoch 121/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0061\n",
      "Epoch 122/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0313\n",
      "Epoch 123/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0243\n",
      "Epoch 124/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0049\n",
      "Epoch 125/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0010\n",
      "Epoch 126/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0053\n",
      "Epoch 127/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9907\n",
      "Epoch 128/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0123\n",
      "Epoch 129/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9931\n",
      "Epoch 130/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0037\n",
      "Epoch 131/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9835\n",
      "Epoch 132/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0005\n",
      "Epoch 133/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9751\n",
      "Epoch 134/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9522\n",
      "Epoch 135/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9929\n",
      "Epoch 136/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9504\n",
      "Epoch 137/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0007\n",
      "Epoch 138/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9698\n",
      "Epoch 139/500\n",
      "41534/41534 [==============================] - 19s 451us/sample - loss: 15.9893\n",
      "Epoch 140/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9552\n",
      "Epoch 141/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9562\n",
      "Epoch 142/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9939\n",
      "Epoch 143/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9649\n",
      "Epoch 144/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9683\n",
      "Epoch 145/500\n",
      "41534/41534 [==============================] - 19s 448us/sample - loss: 15.9788\n",
      "Epoch 146/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.9515\n",
      "Epoch 147/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9345\n",
      "Epoch 148/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9676\n",
      "Epoch 149/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9539\n",
      "Epoch 150/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9295\n",
      "Epoch 151/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9668\n",
      "Epoch 152/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9499\n",
      "Epoch 153/500\n",
      "41534/41534 [==============================] - 19s 445us/sample - loss: 15.9482\n",
      "Epoch 154/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9371\n",
      "Epoch 155/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9217\n",
      "Epoch 156/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9486\n",
      "Epoch 157/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9307\n",
      "Epoch 158/500\n",
      "41534/41534 [==============================] - 19s 448us/sample - loss: 15.9258\n",
      "Epoch 159/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9515\n",
      "Epoch 160/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9427\n",
      "Epoch 161/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.9140\n",
      "Epoch 162/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9234\n",
      "Epoch 163/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9468\n",
      "Epoch 164/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.9284\n",
      "Epoch 165/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.9443\n",
      "Epoch 166/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9111\n",
      "Epoch 167/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9614\n",
      "Epoch 168/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 15.8993\n",
      "Epoch 169/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9248\n",
      "Epoch 170/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9242\n",
      "Epoch 171/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9344\n",
      "Epoch 172/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9253\n",
      "Epoch 173/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9069\n",
      "Epoch 174/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9158\n",
      "Epoch 175/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 15.9293\n",
      "Epoch 176/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9131\n",
      "Epoch 177/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9301\n",
      "Epoch 178/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8838\n",
      "Epoch 179/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 15.9130\n",
      "Epoch 180/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9066\n",
      "Epoch 181/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9112\n",
      "Epoch 182/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9142\n",
      "Epoch 183/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9109\n",
      "Epoch 184/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8833\n",
      "Epoch 185/500\n",
      "41534/41534 [==============================] - 22s 530us/sample - loss: 15.8725\n",
      "Epoch 186/500\n",
      "41534/41534 [==============================] - 22s 527us/sample - loss: 15.8845\n",
      "Epoch 187/500\n",
      "41534/41534 [==============================] - 21s 512us/sample - loss: 15.9028\n",
      "Epoch 188/500\n",
      "41534/41534 [==============================] - 21s 510us/sample - loss: 15.9249\n",
      "Epoch 189/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8861\n",
      "Epoch 190/500\n",
      "41534/41534 [==============================] - 19s 453us/sample - loss: 15.9129\n",
      "Epoch 191/500\n",
      "41534/41534 [==============================] - 21s 494us/sample - loss: 15.9149\n",
      "Epoch 192/500\n",
      "41534/41534 [==============================] - 19s 463us/sample - loss: 15.9082\n",
      "Epoch 193/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8770\n",
      "Epoch 194/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8646\n",
      "Epoch 195/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8875\n",
      "Epoch 196/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9008\n",
      "Epoch 197/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8898\n",
      "Epoch 198/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9115\n",
      "Epoch 199/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9106\n",
      "Epoch 200/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8616\n",
      "Epoch 201/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8853\n",
      "Epoch 202/500\n",
      "41534/41534 [==============================] - 19s 452us/sample - loss: 15.8849\n",
      "Epoch 203/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8859\n",
      "Epoch 204/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9119\n",
      "Epoch 205/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8505\n",
      "Epoch 206/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8753\n",
      "Epoch 207/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8973\n",
      "Epoch 208/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8969\n",
      "Epoch 209/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8991\n",
      "Epoch 210/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8471\n",
      "Epoch 211/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8753\n",
      "Epoch 212/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8862\n",
      "Epoch 213/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9159\n",
      "Epoch 214/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9002\n",
      "Epoch 215/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8987\n",
      "Epoch 216/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8752\n",
      "Epoch 217/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8927\n",
      "Epoch 218/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9011\n",
      "Epoch 219/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8731\n",
      "Epoch 220/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8781\n",
      "Epoch 221/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8677\n",
      "Epoch 222/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8680\n",
      "Epoch 223/500\n",
      "41534/41534 [==============================] - 19s 445us/sample - loss: 15.8893\n",
      "Epoch 224/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8955\n",
      "Epoch 225/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8352\n",
      "Epoch 226/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8642\n",
      "Epoch 227/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8658\n",
      "Epoch 228/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8761\n",
      "Epoch 229/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8718\n",
      "Epoch 230/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8788\n",
      "Epoch 231/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8922\n",
      "Epoch 232/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8730\n",
      "Epoch 233/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8722\n",
      "Epoch 234/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8552\n",
      "Epoch 235/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8607\n",
      "Epoch 236/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8782\n",
      "Epoch 237/500\n",
      "41534/41534 [==============================] - 19s 445us/sample - loss: 15.8812\n",
      "Epoch 238/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8547\n",
      "Epoch 239/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9010\n",
      "Epoch 240/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9286\n",
      "Epoch 241/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8605\n",
      "Epoch 242/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8797\n",
      "Epoch 243/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8650\n",
      "Epoch 244/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.8709\n",
      "Epoch 245/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8658\n",
      "Epoch 246/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8898\n",
      "Epoch 247/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8839\n",
      "Epoch 248/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8603\n",
      "Epoch 249/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.8853\n",
      "Epoch 250/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8736\n",
      "Epoch 251/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9098\n",
      "Epoch 252/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.8589\n",
      "Epoch 253/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8897\n",
      "Epoch 254/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.8708\n",
      "Epoch 255/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8793\n",
      "Epoch 256/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 15.9054\n",
      "Epoch 257/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9182\n",
      "Epoch 258/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9004\n",
      "Epoch 259/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.8828\n",
      "Epoch 260/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8805\n",
      "Epoch 261/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8915\n",
      "Epoch 262/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8909\n",
      "Epoch 263/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8849\n",
      "Epoch 264/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8658\n",
      "Epoch 265/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.8901\n",
      "Epoch 266/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9074\n",
      "Epoch 267/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8706\n",
      "Epoch 268/500\n",
      "41534/41534 [==============================] - 20s 486us/sample - loss: 15.8869\n",
      "Epoch 269/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9025\n",
      "Epoch 270/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8947\n",
      "Epoch 271/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9184\n",
      "Epoch 272/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.8526\n",
      "Epoch 273/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9155\n",
      "Epoch 274/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8949\n",
      "Epoch 275/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9048\n",
      "Epoch 276/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9617\n",
      "Epoch 277/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8967\n",
      "Epoch 278/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8837\n",
      "Epoch 279/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 15.8732\n",
      "Epoch 280/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9479\n",
      "Epoch 281/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8854\n",
      "Epoch 282/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8636\n",
      "Epoch 283/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8670\n",
      "Epoch 284/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8788\n",
      "Epoch 285/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8796\n",
      "Epoch 286/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8866\n",
      "Epoch 287/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8992\n",
      "Epoch 288/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9042\n",
      "Epoch 289/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8707\n",
      "Epoch 290/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8833\n",
      "Epoch 291/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8918\n",
      "Epoch 292/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9521\n",
      "Epoch 293/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9138\n",
      "Epoch 294/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9094\n",
      "Epoch 295/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9243\n",
      "Epoch 296/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8929\n",
      "Epoch 297/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.8713\n",
      "Epoch 298/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9061\n",
      "Epoch 299/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9011\n",
      "Epoch 300/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.8988\n",
      "Epoch 301/500\n",
      "41534/41534 [==============================] - 19s 455us/sample - loss: 15.8791\n",
      "Epoch 302/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9333\n",
      "Epoch 303/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9175\n",
      "Epoch 304/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.8850\n",
      "Epoch 305/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9086\n",
      "Epoch 306/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9088\n",
      "Epoch 307/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9230\n",
      "Epoch 308/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9106\n",
      "Epoch 309/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8921\n",
      "Epoch 310/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9318\n",
      "Epoch 311/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9322\n",
      "Epoch 312/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9021\n",
      "Epoch 313/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9328\n",
      "Epoch 314/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9286\n",
      "Epoch 315/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9528\n",
      "Epoch 316/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9411\n",
      "Epoch 317/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9553\n",
      "Epoch 318/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9110\n",
      "Epoch 319/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.8819\n",
      "Epoch 320/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9044\n",
      "Epoch 321/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9558\n",
      "Epoch 322/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9480\n",
      "Epoch 323/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9037\n",
      "Epoch 324/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9380\n",
      "Epoch 325/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.8776\n",
      "Epoch 326/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9619\n",
      "Epoch 327/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9404\n",
      "Epoch 328/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9361\n",
      "Epoch 329/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9033\n",
      "Epoch 330/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9149\n",
      "Epoch 331/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9187\n",
      "Epoch 332/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9382\n",
      "Epoch 333/500\n",
      "41534/41534 [==============================] - 19s 448us/sample - loss: 15.9447\n",
      "Epoch 334/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9307\n",
      "Epoch 335/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.9235\n",
      "Epoch 336/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9535\n",
      "Epoch 337/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9598\n",
      "Epoch 338/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9347\n",
      "Epoch 339/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9379\n",
      "Epoch 340/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 15.9717\n",
      "Epoch 341/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9997\n",
      "Epoch 342/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 15.9814\n",
      "Epoch 343/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9563\n",
      "Epoch 344/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 15.9848\n",
      "Epoch 345/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9398\n",
      "Epoch 346/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9506\n",
      "Epoch 347/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9109\n",
      "Epoch 348/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9621\n",
      "Epoch 349/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9599\n",
      "Epoch 350/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9978\n",
      "Epoch 351/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9491\n",
      "Epoch 352/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 15.9350\n",
      "Epoch 353/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 15.9758\n",
      "Epoch 354/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9727\n",
      "Epoch 355/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9459\n",
      "Epoch 356/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9379\n",
      "Epoch 357/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 15.9228\n",
      "Epoch 358/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9660\n",
      "Epoch 359/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0092\n",
      "Epoch 360/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9493\n",
      "Epoch 361/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9873\n",
      "Epoch 362/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9242\n",
      "Epoch 363/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9981\n",
      "Epoch 364/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9701\n",
      "Epoch 365/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9510\n",
      "Epoch 366/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9569\n",
      "Epoch 367/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9940\n",
      "Epoch 368/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9618\n",
      "Epoch 369/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9700\n",
      "Epoch 370/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9900\n",
      "Epoch 371/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9660\n",
      "Epoch 372/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9524\n",
      "Epoch 373/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9721\n",
      "Epoch 374/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0587\n",
      "Epoch 375/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9800\n",
      "Epoch 376/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9459\n",
      "Epoch 377/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9851\n",
      "Epoch 378/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9774\n",
      "Epoch 379/500\n",
      "41534/41534 [==============================] - 19s 445us/sample - loss: 15.9563\n",
      "Epoch 380/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9869\n",
      "Epoch 381/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9986\n",
      "Epoch 382/500\n",
      "41534/41534 [==============================] - 21s 512us/sample - loss: 15.9918\n",
      "Epoch 383/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 16.0060\n",
      "Epoch 384/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9671\n",
      "Epoch 385/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0128\n",
      "Epoch 386/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0244\n",
      "Epoch 387/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9802\n",
      "Epoch 388/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.0926\n",
      "Epoch 389/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9823\n",
      "Epoch 390/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9568\n",
      "Epoch 391/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0501\n",
      "Epoch 392/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.0096\n",
      "Epoch 393/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9817\n",
      "Epoch 394/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 15.9888\n",
      "Epoch 395/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0124\n",
      "Epoch 396/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.0068\n",
      "Epoch 397/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9903\n",
      "Epoch 398/500\n",
      "41534/41534 [==============================] - 19s 454us/sample - loss: 16.0812\n",
      "Epoch 399/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9622\n",
      "Epoch 400/500\n",
      "41534/41534 [==============================] - 19s 445us/sample - loss: 16.0107\n",
      "Epoch 401/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0249\n",
      "Epoch 402/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0152\n",
      "Epoch 403/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9762\n",
      "Epoch 404/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 16.0339\n",
      "Epoch 405/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 15.9847\n",
      "Epoch 406/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9911\n",
      "Epoch 407/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 15.9690\n",
      "Epoch 408/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.0447\n",
      "Epoch 409/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 15.9902\n",
      "Epoch 410/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0070\n",
      "Epoch 411/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 16.0490\n",
      "Epoch 412/500\n",
      "41534/41534 [==============================] - 19s 445us/sample - loss: 16.0479\n",
      "Epoch 413/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0459\n",
      "Epoch 414/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0446\n",
      "Epoch 415/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.0393\n",
      "Epoch 416/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0568\n",
      "Epoch 417/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.0171\n",
      "Epoch 418/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 16.0123\n",
      "Epoch 419/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.0247\n",
      "Epoch 420/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.0122\n",
      "Epoch 421/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0049\n",
      "Epoch 422/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.0527\n",
      "Epoch 423/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0759\n",
      "Epoch 424/500\n",
      "41534/41534 [==============================] - 19s 448us/sample - loss: 16.0837\n",
      "Epoch 425/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.0425\n",
      "Epoch 426/500\n",
      "41534/41534 [==============================] - 19s 445us/sample - loss: 16.0761\n",
      "Epoch 427/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0371\n",
      "Epoch 428/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.0669\n",
      "Epoch 429/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0609\n",
      "Epoch 430/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0382\n",
      "Epoch 431/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0363\n",
      "Epoch 432/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0799\n",
      "Epoch 433/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.0681\n",
      "Epoch 434/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0740\n",
      "Epoch 435/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1204\n",
      "Epoch 436/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0855\n",
      "Epoch 437/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0515\n",
      "Epoch 438/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0752\n",
      "Epoch 439/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.0460\n",
      "Epoch 440/500\n",
      "41534/41534 [==============================] - 18s 439us/sample - loss: 16.1105\n",
      "Epoch 441/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1081\n",
      "Epoch 442/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1332\n",
      "Epoch 443/500\n",
      "41534/41534 [==============================] - 18s 440us/sample - loss: 16.0590\n",
      "Epoch 444/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.0586\n",
      "Epoch 445/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1248\n",
      "Epoch 446/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0027\n",
      "Epoch 447/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0633\n",
      "Epoch 448/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0656\n",
      "Epoch 449/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.0712\n",
      "Epoch 450/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0608\n",
      "Epoch 451/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1175\n",
      "Epoch 452/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0404\n",
      "Epoch 453/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0613\n",
      "Epoch 454/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.0625\n",
      "Epoch 455/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0834\n",
      "Epoch 456/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.1438\n",
      "Epoch 457/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.1025\n",
      "Epoch 458/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1404\n",
      "Epoch 459/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.0698\n",
      "Epoch 460/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.3770\n",
      "Epoch 461/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.0440\n",
      "Epoch 462/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1150\n",
      "Epoch 463/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.1485\n",
      "Epoch 464/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.1160\n",
      "Epoch 465/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1020\n",
      "Epoch 466/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1400\n",
      "Epoch 467/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1268\n",
      "Epoch 468/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1715\n",
      "Epoch 469/500\n",
      "41534/41534 [==============================] - 19s 448us/sample - loss: 16.1609\n",
      "Epoch 470/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1315\n",
      "Epoch 471/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1443\n",
      "Epoch 472/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.1466\n",
      "Epoch 473/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.1134\n",
      "Epoch 474/500\n",
      "41534/41534 [==============================] - 19s 447us/sample - loss: 16.1790\n",
      "Epoch 475/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.2088\n",
      "Epoch 476/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1551\n",
      "Epoch 477/500\n",
      "41534/41534 [==============================] - 19s 446us/sample - loss: 16.1840\n",
      "Epoch 478/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1661\n",
      "Epoch 479/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.2119\n",
      "Epoch 480/500\n",
      "41534/41534 [==============================] - 18s 445us/sample - loss: 16.2423\n",
      "Epoch 481/500\n",
      "41534/41534 [==============================] - 18s 442us/sample - loss: 16.1741\n",
      "Epoch 482/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1721\n",
      "Epoch 483/500\n",
      "41534/41534 [==============================] - 19s 448us/sample - loss: 16.1678\n",
      "Epoch 484/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.1935\n",
      "Epoch 485/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.1606\n",
      "Epoch 486/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1858\n",
      "Epoch 487/500\n",
      "41534/41534 [==============================] - 18s 444us/sample - loss: 16.2294\n",
      "Epoch 488/500\n",
      "41534/41534 [==============================] - 18s 441us/sample - loss: 16.1818\n",
      "Epoch 489/500\n",
      "41534/41534 [==============================] - 18s 443us/sample - loss: 16.1896\n",
      "Epoch 490/500\n",
      "41534/41534 [==============================] - 20s 471us/sample - loss: 16.2618\n",
      "Epoch 491/500\n",
      "41534/41534 [==============================] - 20s 475us/sample - loss: 16.2545\n",
      "Epoch 492/500\n",
      "41534/41534 [==============================] - 20s 479us/sample - loss: 16.3025\n",
      "Epoch 493/500\n",
      "41534/41534 [==============================] - 20s 482us/sample - loss: 16.2628\n",
      "Epoch 494/500\n",
      "41534/41534 [==============================] - 20s 481us/sample - loss: 16.1955\n",
      "Epoch 495/500\n",
      "41534/41534 [==============================] - 20s 489us/sample - loss: 16.2522\n",
      "Epoch 496/500\n",
      "41534/41534 [==============================] - 20s 483us/sample - loss: 16.2258\n",
      "Epoch 497/500\n",
      "41534/41534 [==============================] - 20s 482us/sample - loss: 16.2376\n",
      "Epoch 498/500\n",
      "41534/41534 [==============================] - 20s 480us/sample - loss: 16.1931\n",
      "Epoch 499/500\n",
      "41534/41534 [==============================] - 20s 482us/sample - loss: 16.2008\n",
      "Epoch 500/500\n",
      "41534/41534 [==============================] - 20s 479us/sample - loss: 16.2385\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(learning_rate=0.0001, clipvalue=0.5)\n",
    "opt = RMSprop(learning_rate=0.0001)\n",
    "\n",
    "vae_model.compile(optimizer=opt, loss=vae_loss)\n",
    "vae_model.summary()\n",
    "\n",
    "results = vae_model.fit(X_train, X_train,\n",
    "                        shuffle=True,\n",
    "                        epochs=500,\n",
    "                        batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results.history['loss'])\n",
    "#plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manero/DL/lib/python3.8/site-packages/keras/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "X_train_pred = vae_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg error 0.03758674937027951\n",
      "median error 0.03660850675639664\n",
      "99Q: 0.06597148633251958\n",
      "setting threshold on 0.06597148633251958 \n",
      "0.07110014312123288\n"
     ]
    }
   ],
   "source": [
    "mae_vector = get_error_term(X_train_pred, X_train, _rmse=False)\n",
    "print(f'Avg error {np.mean(mae_vector)}\\nmedian error {np.median(mae_vector)}\\n99Q: {np.quantile(mae_vector, 0.99)}')\n",
    "print(f'setting threshold on { np.quantile(mae_vector, 0.99)} ')\n",
    "\n",
    "error_thresh = np.quantile(mae_vector, 0.99)\n",
    "error_thresh = np.quantile(mae_vector, 0.995)\n",
    "print(error_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total non_ignitions 1000000\n",
      "number of possible_ignitions 124324\n"
     ]
    }
   ],
   "source": [
    "X_pred = vae_model.predict(X_test)\n",
    "mae_vector = get_error_term(X_pred, X_test, _rmse=False)\n",
    "anomalies = (mae_vector > error_thresh)\n",
    "\n",
    "np.count_nonzero(anomalies) / len(anomalies)\n",
    "print('total non_ignitions',len(X_pred))\n",
    "print('number of almost_ignitions', np.count_nonzero(anomalies))\n",
    "\n",
    "#print('total y_test',len(y_test))\n",
    "#print(np.count_nonzero(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===== program ends here. now create a new label and label non-ignition dataset with new class ===== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08264557, 0.05409973, 0.08828423, ..., 0.15361315, 0.07860929,\n",
       "       0.05910724])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      1.00      0.67     10384\n",
      "         1.0       0.00      0.00      0.00     10384\n",
      "\n",
      "    accuracy                           0.50     20768\n",
      "   macro avg       0.25      0.50      0.33     20768\n",
      "weighted avg       0.25      0.50      0.33     20768\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\darsh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\darsh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\darsh\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, anomalies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 22)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_transform = pca.fit_transform(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison PCA transformations\n",
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,4))\n",
    "fig.suptitle('PCA transformations comparison',size=20)\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=mae_vector, ax=ax1)\n",
    "ax1.set_xlabel('MAE Vector', size=16)\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=anomalies, ax=ax2)\n",
    "ax2.set_xlabel('Anomalies', size = 16)\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=10, hue=y_test, ax=ax3)\n",
    "ax3.set_xlabel('y_test', size = 16)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=mae_vector)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=20, hue=anomalies)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(x=X_transform[:, 0], y=X_transform[:, 1], s=10, hue=y_test)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure\n",
    "error_df = pd.DataFrame({'Reconstruction_error': np.squeeze(mae_vector),\n",
    "                        'True_class': y_test})\n",
    "\n",
    "error_df = error_df.sample(frac=1).reset_index(drop=True)\n",
    "threshold_fixed = 0.06\n",
    "print(error_thresh)\n",
    "groups = error_df.groupby('True_class')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18,6))\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Attack\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction error for normal and attack data\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobal_error = np.squeeze(mae_vector)\n",
    "Attack_error = error_df[error_df['True_class']==1].Reconstruction_error.to_numpy()\n",
    "Normal_error = error_df[error_df['True_class']==0].Reconstruction_error.to_numpy()\n",
    "print(Attack_error.shape, Normal_error.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2,ax3) = plt.subplots(1,3, figsize=(18,6))\n",
    "fig.suptitle('Error distributions comparison between different traffic categories',\n",
    "             size=20)\n",
    "ax1.hist(Normal_error, bins=150, color='green', alpha=0.6)\n",
    "ax1.set_xlabel('Normal error', size=14)\n",
    "ax1.set_ylabel('No of samples', size = 14)\n",
    "ax2.hist(Attack_error, bins=150, color='pink')\n",
    "ax2.set_xlabel('Attack error', size = 14)\n",
    "ax2.set_ylabel('No of samples', size = 14)\n",
    "ax3.hist(Attack_error, bins=150, color='pink')\n",
    "ax3.hist(Normal_error, bins=150, color='green', alpha=0.6)\n",
    "ax3.set_xlabel('Normal and attack', size = 14)\n",
    "ax3.set_ylabel('No of samples', size = 14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "keras               2.12.0\n",
      "matplotlib          3.5.0\n",
      "numpy               1.22.4\n",
      "pandas              1.5.3\n",
      "seaborn             0.11.2\n",
      "session_info        1.0.0\n",
      "sklearn             1.2.1\n",
      "tensorflow          2.12.0\n",
      "-----\n",
      "IPython             7.29.0\n",
      "jupyter_client      8.2.0\n",
      "jupyter_core        5.3.0\n",
      "notebook            6.5.4\n",
      "-----\n",
      "Python 3.8.12 (default, Oct 12 2021, 13:49:34) [GCC 7.5.0]\n",
      "Linux-5.19.0-45-generic-x86_64-with-glibc2.17\n",
      "-----\n",
      "Session information updated at 2023-06-21 08:12\n"
     ]
    }
   ],
   "source": [
    "import session_info\n",
    "session_info.show(html=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
